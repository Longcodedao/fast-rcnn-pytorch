{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f0fe2e-4221-49cc-bcda-390cad3a7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a70592a-3cae-41f4-b851-81a02fa93a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_labels = {0: \"background\", 1: \"aeroplane\",\n",
    "                 2 : \"bicycle\",\n",
    "                 3 : \"bird\",\n",
    "                 4 : \"boat\", \n",
    "                 5: \"bottle\", \n",
    "                 6: \"bus\",\n",
    "                 7: \"car\",\n",
    "                 8: \"cat\",\n",
    "                 9: \"chair\",\n",
    "                 10: \"cow\",\n",
    "                 11: \"diningtable\",\n",
    "                 12: \"dog\",\n",
    "                 13: \"horse\",\n",
    "                 14: \"motorbike\",\n",
    "                 15: \"person\",\n",
    "                 16: \"pottedplant\",\n",
    "                 17: \"sheep\",\n",
    "                 18: \"sofa\",\n",
    "                 19: \"train\",\n",
    "                 20: \"tvmonitor\"}\n",
    "\n",
    "labels_to_idx = {label: idx for idx, label in idx_to_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f898e0-f7f1-4ef2-808e-c4357c56b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "dataset_path = './datasets/VOCdevkit/VOC2007/'\n",
    "\n",
    "images_path = os.path.join(dataset_path, 'JPEGImages')\n",
    "annotation_path = os.path.join(dataset_path, 'Annotations')\n",
    "images_mode = os.path.join(dataset_path, 'ImageSets', 'Layout')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e989aa0-8f5f-4beb-97a8-addbbeb9a7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9963"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(images_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5947ad-1f25-4033-8dee-f7b8fcd9b49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotation': {'filename': '000007.jpg',\n",
      "                'folder': 'VOC2007',\n",
      "                'object': {'bndbox': {'xmax': '500',\n",
      "                                      'xmin': '141',\n",
      "                                      'ymax': '330',\n",
      "                                      'ymin': '50'},\n",
      "                           'difficult': '0',\n",
      "                           'name': 'car',\n",
      "                           'pose': 'Unspecified',\n",
      "                           'truncated': '1'},\n",
      "                'owner': {'flickrid': 'monsieurrompu', 'name': 'Thom Zemanek'},\n",
      "                'segmented': '0',\n",
      "                'size': {'depth': '3', 'height': '333', 'width': '500'},\n",
      "                'source': {'annotation': 'PASCAL VOC2007',\n",
      "                           'database': 'The VOC2007 Database',\n",
      "                           'flickrid': '194179466',\n",
      "                           'image': 'flickr'}}}\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import pprint\n",
    "\n",
    "example_annotation = os.path.join(annotation_path, '000007.xml')\n",
    "\n",
    "# xml_dict = xmltodict.parse(example_annotation)\n",
    "with open(example_annotation, 'rb') as f:\n",
    "    xml_todict = xmltodict.parse(f)\n",
    "    pprint.pprint(xml_todict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e6ccb6-94b4-42c9-a8af-ae6707d52d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bndbox': {'xmax': '215', 'xmin': '123', 'ymax': '195', 'ymin': '155'},\n",
      "  'difficult': '0',\n",
      "  'name': 'sofa',\n",
      "  'pose': 'Unspecified',\n",
      "  'truncated': '0'},\n",
      " {'bndbox': {'xmax': '307', 'xmin': '239', 'ymax': '205', 'ymin': '156'},\n",
      "  'difficult': '0',\n",
      "  'name': 'chair',\n",
      "  'pose': 'Left',\n",
      "  'truncated': '0'}]\n"
     ]
    }
   ],
   "source": [
    "example_annotation = os.path.join(annotation_path, '000003.xml')\n",
    "\n",
    "# xml_dict = xmltodict.parse(example_annotation)\n",
    "with open(example_annotation, 'rb') as f:\n",
    "    xml_todict = xmltodict.parse(f)\n",
    "    # pprint.pprint(xml_todict)\n",
    "    pprint.pprint(xml_todict['annotation']['object'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390b65b6-bfcd-4fee-aae7-cb331819e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(xml_path):\n",
    "    with open(xml_path, 'rb') as f:\n",
    "        annotation_dict = xmltodict.parse(f)\n",
    "        objects =  annotation_dict['annotation']['object']\n",
    "\n",
    "        labels = list()\n",
    "        bndboxes = list()\n",
    "        # print(objects)\n",
    "        if isinstance(objects, dict):\n",
    "            object_name = objects['name']\n",
    "            label = labels_to_idx[object_name]\n",
    "            bndbox = objects['bndbox']\n",
    "            bndbox = [int(bndbox['xmin']), int(bndbox['ymin']), \n",
    "                      int(bndbox['xmax']), int(bndbox['ymax'])]\n",
    "\n",
    "            labels.append(label)\n",
    "            bndboxes.append(bndbox)\n",
    "        else:\n",
    "            for obj in objects:\n",
    "                object_name = obj['name']\n",
    "                label = labels_to_idx[object_name]\n",
    "                bndbox = obj['bndbox']\n",
    "                bndbox = [int(bndbox['xmin']), int(bndbox['ymin']), \n",
    "                          int(bndbox['xmax']), int(bndbox['ymax'])]\n",
    "    \n",
    "                labels.append(label)\n",
    "                bndboxes.append(bndbox)\n",
    "    \n",
    "        return torch.tensor(labels), torch.tensor(bndboxes)\n",
    "\n",
    "\n",
    "labels, bndboxes = extract_xml(example_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e81936-7ab4-4f62-8e9a-32064b6e1719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_iou(boxes1, boxes2):\n",
    "\n",
    "    # Calculate coordinates of intersection area\n",
    "    x1 = max(boxes1[0], boxes2[0])\n",
    "    y1 = max(boxes1[1], boxes2[1])\n",
    "    x2 = min(boxes1[2], boxes2[2])\n",
    "    y2 = min(boxes1[3], boxes2[3])\n",
    "\n",
    "    # print(x1, y1, x2, y2)\n",
    "    # Calculate area of the intersection\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    # print(intersection_area)\n",
    "    \n",
    "    # Calculate area of union\n",
    "    area_box1 = (boxes1[2] - boxes1[0]) * (boxes1[3] - boxes1[1])\n",
    "    area_box2 = (boxes2[2] - boxes2[0]) * (boxes2[3] - boxes2[1])\n",
    "    # print(area_box1)\n",
    "    # print(area_box2)\n",
    "    union_area = area_box1 + area_box2 - intersection_area\n",
    "    # print(union_area)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / union_area\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Example usage:\n",
    "box1 = torch.tensor([351,  82, 463, 545])\n",
    "box2 = torch.tensor([239, 156, 307, 205])\n",
    "iou = calculate_iou(box1, box2)\n",
    "print(\"IoU:\", iou.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01b9b2f3-2488-49ee-bac2-fba7e59fead1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[203, 257, 209, 301],\n",
       "        [ 53,  75,  63, 144],\n",
       "        [152,  64, 255, 196],\n",
       "        ...,\n",
       "        [  0,   0, 426, 311],\n",
       "        [  0,   0, 500, 211],\n",
       "        [  0,   0, 500, 319]], dtype=torch.int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def selective_search(image):\n",
    "     # Create a Selective Search Segmentation object\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    # Set input image for selective search\n",
    "    ss.setBaseImage(image)\n",
    "\n",
    "    # Switch to fast but low recall selective search method\n",
    "    ss.switchToSelectiveSearchQuality()\n",
    "\n",
    "    # Run selective search on the input image\n",
    "    rects = ss.process()\n",
    "\n",
    "    rects[:, 2] += rects[:, 0] \n",
    "    rects[:, 3] += rects[:, 1] \n",
    "    \n",
    "    return torch.tensor(rects)\n",
    "\n",
    "image_3 = os.path.join(images_path, '000003.jpg')\n",
    "image_3 = cv2.imread(image_3)\n",
    "bounding_boxes = selective_search(image_3)\n",
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4738b27a-8888-40bd-8acf-2aed99f4598b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9478, 4])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "911c3aaa-eee7-4314-8f8c-ae3f109d49a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18,  9]),\n",
       " tensor([[123, 155, 215, 195],\n",
       "         [239, 156, 307, 205]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, bndboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2eea7967-cef0-47fb-a2c2-0a3e561f31dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cfddca22-9a6a-4c00-a839-d226a17ce1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at: 5811\n",
      "5811\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  9, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 18, 18,  9, 18,\n",
      "        18, 18,  9,  9,  9, 18, 18, 18, 18, 18])\n"
     ]
    }
   ],
   "source": [
    "set_bndboxes = []\n",
    "resulting_labels = []\n",
    "\n",
    "positive_nums = 16\n",
    "negative_nums = 64 - 16\n",
    "\n",
    "positive_counter = 0\n",
    "negative_counter = 0\n",
    "\n",
    "count = 0\n",
    "\n",
    "for bnd_box in range(bounding_boxes.shape[0]):\n",
    "\n",
    "    if positive_counter >= positive_nums and negative_counter >= negative_nums:\n",
    "        print(f\"Stop at: {count}\")\n",
    "        break\n",
    "    \n",
    "    combination_iou = []\n",
    "    for idx in range(labels.shape[0]):\n",
    "        # print(bounding_boxes[bnd_box])\n",
    "        # print(bndboxes[idx])\n",
    "        iou = calculate_iou(bounding_boxes[bnd_box], bndboxes[idx])\n",
    "        combination_iou.append(iou)\n",
    "    combination_iou = np.array(combination_iou)\n",
    "    # print(combination_iou)\n",
    "    max_iou = np.max(combination_iou)\n",
    "    label = None\n",
    "\n",
    "    \n",
    "    \n",
    "    count += 1\n",
    "    if max_iou >= 0.5 and positive_counter < positive_nums:\n",
    "        element = np.argmax(combination_iou)\n",
    "        label = labels[element]\n",
    "        positive_counter += 1\n",
    "    elif max_iou < 0.5 and max_iou >= 0.1 and negative_counter < negative_nums:\n",
    "        label = 0\n",
    "        negative_counter += 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    resulting_labels.append(label)\n",
    "    set_bndboxes.append(bounding_boxes[bnd_box])\n",
    "    \n",
    "\n",
    "print(count)\n",
    "resulting_labels = torch.tensor(resulting_labels)\n",
    "# set_bndboxes = torch.tensor(set_bndboxes)\n",
    "# print(set_bndboxes)\n",
    "print(resulting_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46b7bb0c-8ca7-45b6-85b9-0c9d61bdf014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 500, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec301383-fafe-4aa2-a612-fbd23760d57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resulting_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21d1bbe2-7fe6-4277-b4b8-45287025ac2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 24,  25,  51, 139, 157, 206, 285, 305, 324, 409, 428, 441, 484, 537,\n",
       "        546, 554, 580, 606, 644, 647, 682, 691, 698, 726, 740, 767, 798, 812,\n",
       "        813, 884, 886, 945])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_labels\n",
    "\n",
    "non_zero_indicies = (resulting_labels != 0 )\n",
    "indicies = torch.nonzero(non_zero_indicies).squeeze()\n",
    "indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7257cbb-55be-4c3f-9d76-e33a0a17bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a95dbea-a43f-48e9-b942-f8cfffd73a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms  as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class PascalVOCDataset(Dataset):    \n",
    "    def __init__(self, dataset_path, transform, R = 64, s = 600, mode = 'train'):    \n",
    "\n",
    "        self.transform = transform\n",
    "        self.s = s\n",
    "        self.R = R  # Number of bounding boxes\n",
    "        images_path = os.path.join(dataset_path, 'JPEGImages')\n",
    "        annotation_path = os.path.join(dataset_path, 'Annotations')\n",
    "\n",
    "        images_mode = os.path.join(dataset_path, 'ImageSets', 'Main')\n",
    "        if mode == 'train':\n",
    "            images_mode = os.path.join(images_mode, 'train.txt')\n",
    "        elif mode == 'val':\n",
    "            images_mode = os.path.join(images_mode, 'val.txt')\n",
    "        elif mode == 'test':\n",
    "            images_mode = os.path.join(images_mode, 'test.txt')\n",
    "\n",
    "        self.images_path = []\n",
    "        self.annotation_path = []\n",
    "        with open(images_mode, 'r') as f:\n",
    "            for file in f.readlines():\n",
    "                file_id = file.strip()\n",
    "                # print(file_id)\n",
    "                file_path = os.path.join(images_path, file_id + \".jpg\")\n",
    "                # print(file_path)\n",
    "                xml_path = os.path.join(annotation_path, file_id + \".xml\")\n",
    "\n",
    "                self.images_path.append(file_path)\n",
    "                self.annotation_path.append(xml_path)\n",
    "                \n",
    "        # self.images_path = sorted(glob.glob(images_path + \"/*.jpg\"))\n",
    "        # self.annotation_path = sorted(glob.glob(images_path + \"/*.xml\"))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotation_path)\n",
    "\n",
    "    # Try to combine Selective Search to to generate 64 Bounding boxes in this stage \n",
    "    def __getitem__(self, index):        \n",
    "        image_file = self.images_path[index]\n",
    "        annotation_file = self.annotation_path[index]\n",
    "            \n",
    "        # try:\n",
    "        #     image = Image.open(image_file)\n",
    "        # except IOError:\n",
    "        #     print(f'Corrupt Image at {index}')\n",
    "        #     if index == len(self) - 1:\n",
    "        #         index = 0\n",
    "        #     return self[index + 1]\n",
    "\n",
    "        image_cv = cv2.imread(image_file)\n",
    "        \n",
    "        if image_cv is None: \n",
    "            print(f'Failed to load image at index {index}')\n",
    "            if index == len(self) - 1:\n",
    "                index = 0\n",
    "            return self[index + 1]\n",
    "            \n",
    "        resize_image_cv = cv2.resize(image_cv, (self.s, self.s))\n",
    "        \n",
    "        \n",
    "        # image = self.transform(image)\n",
    "        bndboxes = extract_xml(annotation_file)\n",
    "\n",
    "        \n",
    "        return {\"image\": image,\n",
    "               \"bndboxes\": bndboxes}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d4766a-0c15-44bd-8dc7-3d628a9e2d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501\n"
     ]
    }
   ],
   "source": [
    "size = 600\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "voc_dataset_train = PascalVOCDataset(dataset_path,\n",
    "                              transform = transform,\n",
    "                              mode = 'train')\n",
    "\n",
    "print(len(voc_dataset_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39f2fc6-8fb5-48fb-8e7a-340ac303b28e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(voc_dataset_train)):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mvoc_dataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbndboxes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m, in \u001b[0;36mPascalVOCDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# image = self.transform(image)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m bndboxes \u001b[38;5;241m=\u001b[39m extract_xml(annotation_file)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mimage\u001b[49m,\n\u001b[1;32m     71\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbndboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: bndboxes}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(voc_dataset_train)):\n",
    "    print(voc_dataset_train[i]['bndboxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92553177-c7e0-44ca-b91a-9776b548aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dataset_val = PascalVOCDataset(dataset_path,\n",
    "                              transform = transform,\n",
    "                              mode = 'val')\n",
    "\n",
    "len(voc_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11854a-b624-45e1-8de2-dfba9a076a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

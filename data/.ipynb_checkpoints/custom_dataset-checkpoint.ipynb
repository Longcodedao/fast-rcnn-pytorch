{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f0fe2e-4221-49cc-bcda-390cad3a7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8f898e0-f7f1-4ef2-808e-c4357c56b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "dataset_path = './datasets/VOCdevkit/VOC2007/'\n",
    "\n",
    "images_path = os.path.join(dataset_path, 'JPEGImages')\n",
    "annotation_path = os.path.join(dataset_path, 'Annotations')\n",
    "images_mode = os.path.join(dataset_path, 'ImageSets', 'Layout')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e989aa0-8f5f-4beb-97a8-addbbeb9a7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9963"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(images_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5947ad-1f25-4033-8dee-f7b8fcd9b49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotation': {'filename': '000007.jpg',\n",
      "                'folder': 'VOC2007',\n",
      "                'object': {'bndbox': {'xmax': '500',\n",
      "                                      'xmin': '141',\n",
      "                                      'ymax': '330',\n",
      "                                      'ymin': '50'},\n",
      "                           'difficult': '0',\n",
      "                           'name': 'car',\n",
      "                           'pose': 'Unspecified',\n",
      "                           'truncated': '1'},\n",
      "                'owner': {'flickrid': 'monsieurrompu', 'name': 'Thom Zemanek'},\n",
      "                'segmented': '0',\n",
      "                'size': {'depth': '3', 'height': '333', 'width': '500'},\n",
      "                'source': {'annotation': 'PASCAL VOC2007',\n",
      "                           'database': 'The VOC2007 Database',\n",
      "                           'flickrid': '194179466',\n",
      "                           'image': 'flickr'}}}\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import pprint\n",
    "\n",
    "example_annotation = os.path.join(annotation_path, '000007.xml')\n",
    "\n",
    "# xml_dict = xmltodict.parse(example_annotation)\n",
    "with open(example_annotation, 'rb') as f:\n",
    "    xml_todict = xmltodict.parse(f)\n",
    "    pprint.pprint(xml_todict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e6ccb6-94b4-42c9-a8af-ae6707d52d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bndbox': {'xmax': '215', 'xmin': '123', 'ymax': '195', 'ymin': '155'},\n",
      "  'difficult': '0',\n",
      "  'name': 'sofa',\n",
      "  'pose': 'Unspecified',\n",
      "  'truncated': '0'},\n",
      " {'bndbox': {'xmax': '307', 'xmin': '239', 'ymax': '205', 'ymin': '156'},\n",
      "  'difficult': '0',\n",
      "  'name': 'chair',\n",
      "  'pose': 'Left',\n",
      "  'truncated': '0'}]\n"
     ]
    }
   ],
   "source": [
    "example_annotation = os.path.join(annotation_path, '000003.xml')\n",
    "\n",
    "# xml_dict = xmltodict.parse(example_annotation)\n",
    "with open(example_annotation, 'rb') as f:\n",
    "    xml_todict = xmltodict.parse(f)\n",
    "    # pprint.pprint(xml_todict)\n",
    "    pprint.pprint(xml_todict['annotation']['object'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a70592a-3cae-41f4-b851-81a02fa93a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_labels = {0: \"background\", 1: \"aeroplane\",\n",
    "                 2 : \"bicycle\",\n",
    "                 3 : \"bird\",\n",
    "                 4 : \"boat\", \n",
    "                 5: \"bottle\", \n",
    "                 6: \"bus\",\n",
    "                 7: \"car\",\n",
    "                 8: \"cat\",\n",
    "                 9: \"chair\",\n",
    "                 10: \"cow\",\n",
    "                 11: \"diningtable\",\n",
    "                 12: \"dog\",\n",
    "                 13: \"horse\",\n",
    "                 14: \"motorbike\",\n",
    "                 15: \"person\",\n",
    "                 16: \"pottedplant\",\n",
    "                 17: \"sheep\",\n",
    "                 18: \"sofa\",\n",
    "                 19: \"train\",\n",
    "                 20: \"tvmonitor\"}\n",
    "\n",
    "labels_to_idx = {label: idx for idx, label in idx_to_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "390b65b6-bfcd-4fee-aae7-cb331819e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'sofa', 'pose': 'Unspecified', 'truncated': '0', 'difficult': '0', 'bndbox': {'xmin': '123', 'ymin': '155', 'xmax': '215', 'ymax': '195'}}\n",
      "{'name': 'chair', 'pose': 'Left', 'truncated': '0', 'difficult': '0', 'bndbox': {'xmin': '239', 'ymin': '156', 'xmax': '307', 'ymax': '205'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(18, [123, 155, 215, 195]), (9, [239, 156, 307, 205])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_xml(xml_path):\n",
    "    with open(xml_path, 'rb') as f:\n",
    "        annotation_dict = xmltodict.parse(f)\n",
    "        objects =  annotation_dict['annotation']['object']\n",
    "\n",
    "        labels = list()\n",
    "        # print(objects)\n",
    "        if len(objects) == 1:\n",
    "            object_name = objects['name']\n",
    "            label = labels_to_idx[object_name]\n",
    "            bndbox = objects['bndbox']\n",
    "            bndbox = [int(bndbox['xmin']), int(bndbox['ymin']), \n",
    "                      int(bndbox['xmax']), int(bndbox['ymax'])]\n",
    "\n",
    "            labels.append((label, bndbox))\n",
    "        else:\n",
    "            for obj in objects:\n",
    "                object_name = obj['name']\n",
    "                label = labels_to_idx[object_name]\n",
    "                bndbox = obj['bndbox']\n",
    "                bndbox = [int(bndbox['xmin']), int(bndbox['ymin']), \n",
    "                          int(bndbox['xmax']), int(bndbox['ymax'])]\n",
    "    \n",
    "                labels.append((label, bndbox))\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "extract_xml(example_annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a95dbea-a43f-48e9-b942-f8cfffd73a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms  as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, transform, mode = 'train'):    \n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        images_path = os.path.join(dataset_path, 'JPEGImages')\n",
    "        annotation_path = os.path.join(dataset_path, 'Annotations')\n",
    "\n",
    "        images_mode = os.path.join(dataset_path, 'ImageSets', 'Main')\n",
    "        if mode == 'train':\n",
    "            images_mode = os.path.join(images_mode, 'train.txt')\n",
    "        elif mode == 'val':\n",
    "            images_mode = os.path.join(images_mode, 'val.txt')\n",
    "        elif mode == 'test':\n",
    "            images_mode = os.path.join(images_mode, 'test.txt')\n",
    "\n",
    "        self.images_path = []\n",
    "        self.annotation_path = []\n",
    "        with open(images_mode, 'r') as f:\n",
    "            for file in f.readlines():\n",
    "                file_id = file.strip()\n",
    "                # print(file_id)\n",
    "                file_path = os.path.join(images_path, file_id + \".jpg\")\n",
    "                # print(file_path)\n",
    "                xml_path = os.path.join(annotation_path, file_id + \".xml\")\n",
    "\n",
    "                self.images_path.append(file_path)\n",
    "                self.annotation_path.append(xml_path)\n",
    "                \n",
    "        # self.images_path = sorted(glob.glob(images_path + \"/*.jpg\"))\n",
    "        # self.annotation_path = sorted(glob.glob(images_path + \"/*.xml\"))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotation_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.images_path[index]\n",
    "        annotation_file = self.annotation_path[index]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_file)\n",
    "        except IOError:\n",
    "            print(f'Corrupt Image at {index}')\n",
    "            if index == len(self) - 1:\n",
    "                index = 0\n",
    "            return self[index + 1]\n",
    "\n",
    "        image = self.transform(image)\n",
    "        bndboxes = extract_xml(annotation_file)\n",
    "\n",
    "        return {\"image\": image,\n",
    "               \"bndboxes\": bndboxes}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61d4766a-0c15-44bd-8dc7-3d628a9e2d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501\n"
     ]
    }
   ],
   "source": [
    "size = 600\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "voc_dataset_train = PascalVOCDataset(dataset_path,\n",
    "                              transform = transform,\n",
    "                              mode = 'train')\n",
    "\n",
    "print(len(voc_dataset_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b39f2fc6-8fb5-48fb-8e7a-340ac303b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvoc_dataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 55\u001b[0m, in \u001b[0;36mPascalVOCDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     54\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m---> 55\u001b[0m bndboxes \u001b[38;5;241m=\u001b[39m \u001b[43mextract_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image,\n\u001b[1;32m     58\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbndboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: bndboxes}\n",
      "Cell \u001b[0;32mIn[70], line 11\u001b[0m, in \u001b[0;36mextract_xml\u001b[0;34m(xml_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(obj)\n\u001b[0;32m---> 11\u001b[0m     object_name \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m     label \u001b[38;5;241m=\u001b[39m labels_to_idx[object_name]\n\u001b[1;32m     13\u001b[0m     bndbox \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbndbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "voc_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92553177-c7e0-44ca-b91a-9776b548aa89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2510"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dataset_val = PascalVOCDataset(dataset_path,\n",
    "                              transform = transform,\n",
    "                              mode = 'val')\n",
    "\n",
    "len(voc_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11854a-b624-45e1-8de2-dfba9a076a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
